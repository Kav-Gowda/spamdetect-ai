# -*- coding: utf-8 -*-
"""SpamDetect AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nWmzhyX3WOY-E_wpmfZCLH-OgTikiz9t
"""

# --- Install dependencies ---
!pip install -q nltk wordcloud tensorflow>=2.4

import nltk
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download("punkt")

# --- Import Libraries ---
import os, re, collections, pickle, warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

# --- Display and Plot Settings ---
plt.rcParams['figure.figsize'] = (15, 5)
plt.style.use('ggplot')
pd.set_option("precision", 3)
pd.options.display.float_format = '{:.3f}'.format

# --- Misc Setup ---
seed = 42
warnings.filterwarnings("ignore")

# --- Helper Function: Plot Training History ---
def plot_history(history):
    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]
    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]
    acc_list = [s for s in history.history.keys() if 'accuracy' in s and 'val' not in s]
    val_acc_list = [s for s in history.history.keys() if 'accuracy' in s and 'val' in s]

    if len(loss_list) == 0:
        print('Loss is missing in history')
        return

    epochs = range(1, len(history.history[loss_list[0]]) + 1)
    plt.figure(figsize=(12, 5), dpi=100)
    COLOR = 'gray'

    # Loss
    plt.subplot(1, 2, 1)
    for l in loss_list:
        plt.plot(epochs, history.history[l], 'b-o', label=f"Train ({history.history[l][-1]:.4f})")
    for l in val_loss_list:
        plt.plot(epochs, history.history[l], 'g', label=f"Valid ({history.history[l][-1]:.4f})")
    plt.title('Loss')
    plt.xlabel('Epochs')
    plt.legend(facecolor='gray', loc='best')
    plt.grid(True)

    # Accuracy
    plt.subplot(1, 2, 2)
    for l in acc_list:
        plt.plot(epochs, history.history[l], 'b-o', label=f"Train ({history.history[l][-1]:.4f})")
    for l in val_acc_list:
        plt.plot(epochs, history.history[l], 'g', label=f"Valid ({history.history[l][-1]:.4f})")
    plt.title('Accuracy')
    plt.xlabel('Epochs')
    plt.legend(facecolor='gray', loc='best')
    plt.grid(True)

    plt.tight_layout()
    plt.show()

# --- Helper Function: Plot Confusion Matrix ---
def plot_conf_matr(conf_matr, classes, normalize=False,
                   title='Confusion Matrix', cmap=plt.cm.winter):
    import itertools
    accuracy = np.trace(conf_matr) / np.sum(conf_matr).astype('float')
    sns.set(font_scale=1.4)

    plt.figure(figsize=(12, 8))
    plt.imshow(conf_matr, interpolation='nearest', cmap=cmap)
    plt.title('\n' + title + '\n')
    plt.colorbar()

    if classes is not None:
        tick_marks = np.arange(len(classes))
        plt.xticks(tick_marks, classes, rotation=45)
        plt.yticks(tick_marks, classes)

    if normalize:
        conf_matr = conf_matr.astype('float') / conf_matr.sum(axis=1)[:, np.newaxis]

    thresh = conf_matr.max() / 1.5 if normalize else conf_matr.max() / 2
    for i, j in itertools.product(range(conf_matr.shape[0]), range(conf_matr.shape[1])):
        text = f"{conf_matr[i, j]*100:.2f}%" if normalize else f"{conf_matr[i, j]:,}"
        plt.text(j, i, text, horizontalalignment="center", fontweight='bold',
                 color="white" if conf_matr[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel(f'Predicted label\n\nAccuracy = {accuracy*100:.2f}%; Error = {(1-accuracy)*100:.2f}%')
    plt.show()

# --- Helper Functions: Word Analysis ---
def plot_words(text_series, number):
    words_counter = collections.Counter(
        [word for sentence in text_series for word in sentence.split()]
    )
    most_common = words_counter.most_common(number)
    most_df = pd.DataFrame(most_common, columns=["Words", "Amount"]).sort_values(by="Amount")
    ax = most_df.plot.barh(x="Words", y="Amount", color="blue", figsize=(10, 15))
    for i, v in enumerate(most_df["Amount"]):
        plt.text(v, i, " " + str(v), color='black', va='center', fontweight='bold')
    plt.tight_layout()


def word_cloud(tag):
    df_words_nl = ' '.join(list(df_spam[df_spam['feature'] == tag]['message']))
    df_wc_nl = WordCloud(width=600, height=512).generate(df_words_nl)
    plt.figure(figsize=(13, 9), facecolor='k')
    plt.imshow(df_wc_nl)
    plt.axis('off')
    plt.tight_layout(pad=1)
    plt.show()

# Reading the Dataset
df_spam = pd.read_csv('spam.csv', encoding='latin-1')

# Dataset manipulations & simple EDA
df_spam = df_spam.filter(['v1', 'v2'], axis=1)
df_spam.columns = ['feature', 'message']
df_spam.drop_duplicates(inplace=True, ignore_index=True)

print('Number of null values:\n')
df_spam.isnull().sum()

# Dataset size & feature names + primary statistics (compact)
print(df_spam['feature'].value_counts(), '\n')
print(df_spam.shape, df_spam.columns, '\n')
df_spam.describe().T

# Class distribution with frequencies (%)
plt.figure(figsize=(10, 6))
counter = df_spam.shape[0]
ax1 = sns.countplot(x='feature', data=df_spam)
ax2 = ax1.twinx()                      # Double axis
ax2.yaxis.tick_left()                  # Put % axis on the left
ax1.yaxis.tick_right()
ax1.yaxis.set_label_position('right')
ax2.yaxis.set_label_position('left')
ax2.set_ylabel('frequency, %')

for p in ax1.patches:
    x = p.get_bbox().get_points()[:, 0]
    y = p.get_bbox().get_points()[1, 1]
    ax1.annotate('{:.2f}%'.format(100. * y / counter),
                 (x.mean(), y),
                 ha='center', va='bottom')

# Ticks and ranges
ax1.yaxis.set_major_locator(ticker.LinearLocator(11))
ax2.set_ylim(0, 100)
ax1.set_ylim(0, counter)
ax2.yaxis.set_major_locator(ticker.MultipleLocator(10))
ax2.grid(None)
plt.show()

# Top words (simple frequency view)
plot_words(df_spam['message'], number=30)

# Word clouds by class
word_cloud('spam')
word_cloud('ham')

# Hyperparameters (tunable within noted ranges)
size_vocabulary   = 1000   # choose in [500 .. 1500], divisible by 500
embedding_dimension = 64   # choose in [32 .. 256], divisible by 32
trunc_type        = 'post'
padding_type      = 'post'
threshold         = 0.5    # choose in [0 .. 1]
oov_token         = "<OOV>"
test_size, valid_size = 0.05, 0.2
num_epochs        = 20     # choose in [20 .. 50], divisible by 5
drop_level        = 0.3    # choose in [0 .. 1]

print("\t\tStage I. Preliminary actions — preparing text data\n")

full_df_l = []
lemmatizer = WordNetLemmatizer()

for i in range(df_spam.shape[0]):
    msg = df_spam.iloc[i, 1]
    msg = re.sub(r'\b[\w\-.]+?@\w+?\.\w{2,4}\b', 'emailaddr', msg)                  # replace email addresses
    msg = re.sub(r'(http[s]?\S+)|(\w+\.[A-Za-z]{2,4}\S*)', 'httpaddr', msg)         # replace URLs
    msg = re.sub(r'£|\$', 'moneysymb', msg)                                         # replace currency symbols
    msg = re.sub(r'\b(\+\d{1,2}\s)?\d?[\-(.]?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}\b',
                 'phonenumbr', msg)                                                 # replace phone numbers
    msg = re.sub(r'\d+(\.\d+)?', 'numbr', msg)                                      # replace numeric values
    msg = re.sub(r'[^\w\d\s]', ' ', msg)                                            # remove punctuation
    msg = re.sub(r'[^A-Za-z]', ' ', msg).lower()                                    # keep only letters

    tokenized = word_tokenize(msg)
    filtered = [
        lemmatizer.lemmatize(word)
        for word in tokenized
        if word not in set(stopwords.words('english'))
    ]
    cleaned_msg = " ".join(filtered)
    full_df_l.append(cleaned_msg)

# Visualize most frequent words after preprocessing
plot_words(full_df_l, number=35)

# Feature extraction and train-test split
vectorizer = CountVectorizer(max_features=size_vocabulary)
X = vectorizer.fit_transform(full_df_l).toarray()
y = df_spam['feature']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=(test_size + valid_size), random_state=seed
)

print(f"Number of rows in training set: {X_train.shape}")
print(f"Number of rows in test set: {X_test.shape}")

# --- Stage IIa. Gaussian Naive Bayes ---
print("\t\tStage IIa. Gaussian Naive Bayes\n")

class_NBC = GaussianNB().fit(X_train, y_train)
y_pred_NBC = class_NBC.predict(X_test)

print(f"The first two predicted labels: {y_pred_NBC[0]}, {y_pred_NBC[1]}\n")

conf_m_NBC = confusion_matrix(y_test, y_pred_NBC)
class_rep_NBC = classification_report(y_test, y_pred_NBC)

print("\t\t\tClassification Report:\n\n", class_rep_NBC, "\n")

plot_conf_matr(conf_m_NBC, classes=['Spam', 'Ham'], normalize=False,
               title='Confusion Matrix — Gaussian Naive Bayes')

# --- Stage IIb. Multinomial Naive Bayes ---
print("\t\tStage IIb. Multinomial Naive Bayes\n")

class_MNB = MultinomialNB().fit(X_train, y_train)
y_pred_MNB = class_MNB.predict(X_test)

print(f"The first two predicted labels: {y_pred_MNB[0]}, {y_pred_MNB[1]}\n")

conf_m_MNB = confusion_matrix(y_test, y_pred_MNB)
class_rep_MNB = classification_report(y_test, y_pred_MNB)

print("\t\t\tClassification Report:\n\n", class_rep_MNB, "\n")

plot_conf_matr(conf_m_MNB, classes=['Spam', 'Ham'], normalize=False,
               title='Confusion Matrix — Multinomial Naive Bayes')

# --- Stage III. Decision Tree Classifier ---
print("\t\tStage III. Decision Tree Classifier\n")

class_DTC = DecisionTreeClassifier(random_state=seed).fit(X_train, y_train)
y_pred_DTC = class_DTC.predict(X_test)

print(f"The first two predicted labels: {y_pred_DTC[0]}, {y_pred_DTC[1]}\n")

conf_m_DTC = confusion_matrix(y_test, y_pred_DTC)
class_rep_DTC = classification_report(y_test, y_pred_DTC)

print("\t\t\tClassification Report:\n\n", class_rep_DTC, "\n")

plot_conf_matr(conf_m_DTC, classes=['Spam', 'Ham'], normalize=False,
               title='Confusion Matrix — Decision Tree')

# --- Stage IV. Logistic Regression ---
print("\t\tStage IV. Logistic Regression\n")

class_LR = LogisticRegression(random_state=seed, solver='liblinear').fit(X_train, y_train)
y_pred_LR = class_LR.predict(X_test)

print(f"The first two predicted labels: {y_pred_LR[0]}, {y_pred_LR[1]}\n")

conf_m_LR = confusion_matrix(y_test, y_pred_LR)
class_rep_LR = classification_report(y_test, y_pred_LR)

print("\t\t\tClassification Report:\n\n", class_rep_LR, "\n")

plot_conf_matr(conf_m_LR, classes=['Spam', 'Ham'], normalize=False,
               title='Confusion Matrix — Logistic Regression')

# --- Stage V. KNeighbors Classifier ---
print("\t\tStage V. KNeighbors Classifier\n")

class_KNC = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)
y_pred_KNC = class_KNC.predict(X_test)

print(f"The first two predicted labels: {y_pred_KNC[0]}, {y_pred_KNC[1]}\n")

conf_m_KNC = confusion_matrix(y_test, y_pred_KNC)
class_rep_KNC = classification_report(y_test, y_pred_KNC)

print("\t\t\tClassification Report:\n\n", class_rep_KNC, "\n")

plot_conf_matr(conf_m_KNC, classes=['Spam', 'Ham'], normalize=False,
               title='Confusion Matrix — KNeighbors Classifier')

# --- Stage VI. Support Vector Classification ---
print("\t\tStage VI. Support Vector Classification\n")

class_SVC = SVC(probability=True, random_state=seed).fit(X_train, y_train)
y_pred_SVC = class_SVC.predict(X_test)

print(f"The first two predicted labels: {y_pred_SVC[0]}, {y_pred_SVC[1]}\n")

conf_m_SVC = confusion_matrix(y_test, y_pred_SVC)
class_rep_SVC = classification_report(y_test, y_pred_SVC)

print("\t\t\tClassification Report:\n\n", class_rep_SVC, "\n")

plot_conf_matr(conf_m_SVC, classes=['Spam', 'Ham'], normalize=False,
               title='Confusion Matrix — Support Vector Classifier')

# --- Stage VII. Gradient Boosting Classifier ---
print("\t\tStage VII. Gradient Boosting Classifier\n")

class_GBC = GradientBoostingClassifier(random_state=seed).fit(X_train, y_train)
y_pred_GBC = class_GBC.predict(X_test)

print(f"The first two predicted labels: {y_pred_GBC[0]}, {y_pred_GBC[1]}\n")

conf_m_GBC = confusion_matrix(y_test, y_pred_GBC)
class_rep_GBC = classification_report(y_test, y_pred_GBC)

print("\t\t\tClassification Report:\n\n", class_rep_GBC, "\n")

plot_conf_matr(conf_m_GBC, classes=['Spam', 'Ham'], normalize=False,
               title='Confusion Matrix — Gradient Boosting Classifier')

# --- Stage VIII. Bagging Classifier ---
print("\t\tStage VIII. Bagging Classifier + SVC Base Estimator\n")

base_model = SVC(probability=True, random_state=seed)
class_BC = BaggingClassifier(estimator=base_model, random_state=seed).fit(X_train, y_train)
y_pred_BC = class_BC.predict(X_test)

print(f"The first two predicted labels: {y_pred_BC[0]}, {y_pred_BC[1]}\n")

conf_m_BC = confusion_matrix(y_test, y_pred_BC)
class_rep_BC = classification_report(y_test, y_pred_BC)

print("\t\t\tClassification Report:\n\n", class_rep_BC, "\n")

plot_conf_matr(conf_m_BC, classes=['Spam', 'Ham'], normalize=False,
               title='Confusion Matrix — Bagging Classifier (with SVC base)')

# I stage. Preliminary actions — preparing needed sets
print("Stage I. Preliminary actions. Preparing of needed sets\n")

sentences_new_set = []
labels_new_set = []
for i in range(df_spam.shape[0]):
    sentences_new_set.append(df_spam['message'][i])
    labels_new_set.append(df_spam['feature'][i])

# Train/Valid/Test split boundaries (by index slices)
train_size  = int(df_spam.shape[0] * (1 - test_size - valid_size))
valid_bound = int(df_spam.shape[0] * (1 - valid_size))

train_sentences = sentences_new_set[0:train_size]
valid_sentences = sentences_new_set[train_size:valid_bound]
test_sentences  = sentences_new_set[valid_bound:]

train_labels_str = labels_new_set[0:train_size]
valid_labels_str = labels_new_set[train_size:valid_bound]
test_labels_str  = labels_new_set[valid_bound:]

# II stage. Labels transformations
print("Stage II. Labels transformations\n")

train_labels = [1 if x == 'ham' else 0 for x in train_labels_str]
valid_labels = [1 if x == 'ham' else 0 for x in valid_labels_str]
test_labels  = [1 if x == 'ham' else 0 for x in test_labels_str]

train_labels = np.array(train_labels)
valid_labels = np.array(valid_labels)
test_labels  = np.array(test_labels)

# III stage. Tokenization (fit on train only)
print("Stage III. Tokenization\n")

tokenizer = Tokenizer(num_words=size_vocabulary,
                      oov_token=oov_token,
                      lower=False)
tokenizer.fit_on_texts(train_sentences)
word_index = tokenizer.word_index

# Sequences + padding
train_sequences = tokenizer.texts_to_sequences(train_sentences)
size_voc = len(word_index) + 1
max_len = max(len(s) for s in train_sequences)

train_set = pad_sequences(train_sequences,
                          padding=padding_type,
                          maxlen=max_len,
                          truncating=trunc_type)

valid_sequences = tokenizer.texts_to_sequences(valid_sentences)
valid_set = pad_sequences(valid_sequences,
                          padding=padding_type,
                          maxlen=max_len,
                          truncating=trunc_type)

test_sequences = tokenizer.texts_to_sequences(test_sentences)
test_set = pad_sequences(test_sequences,
                         padding=padding_type,
                         maxlen=max_len,
                         truncating=trunc_type)

# IV stage. Model building
print("Stage IV. Model building\n")

model = Sequential([
    Embedding(size_voc, embedding_dimension, input_length=max_len),
    Bidirectional(LSTM(100)),
    Dropout(drop_level),
    Dense(20, activation='relu'),
    Dropout(drop_level),
    Dense(1, activation='sigmoid')
])

# V stage. Model compiling & fitting
print("Stage V. Model compiling & fitting\n")

optim = Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])
model.summary()

history = model.fit(
    train_set,
    train_labels,
    epochs=num_epochs,
    validation_data=(valid_set, valid_labels),
    workers=os.cpu_count(),
    use_multiprocessing=True,
    verbose=1
)

# VI stage. Results visualization
print("Stage VI. Results visualization\n")
plot_history(history)

model_score = model.evaluate(test_set, test_labels, batch_size=embedding_dimension, verbose=1)
print(f"Test accuracy: {model_score[1] * 100:0.2f}% \t\t Test error: {model_score[0]:0.4f}")

# VII stage. Model saving & predict checking
M_name = "My_model"
pickle.dump(tokenizer, open(M_name + ".pkl", "wb"))
filepath = M_name + '.h5'
tf.keras.models.save_model(model, filepath, include_optimizer=True, save_format='h5', overwrite=True)
print("Size of the saved model:", os.stat(filepath).st_size, "bytes")

# Predictions and evaluation (bLSTM)
y_pred_bLSTM = model.predict(test_set)

y_prediction = [1 if p > threshold else 0 for p in y_pred_bLSTM.ravel()]

conf_m_bLSTM = confusion_matrix(test_labels, y_prediction)
class_rep_bLSTM = classification_report(test_labels, y_prediction)
print('\t\t\tClassification report:\n\n', class_rep_bLSTM, '\n')

plot_conf_matr(conf_m_bLSTM, classes=['Spam', 'Ham'], normalize=False,
               title='Confusion matrix for bLSTM')

# Quick single-sentence check (you can change this message)
message_example = ["Darling, please give me a cup of tea"]

message_example_tp = pad_sequences(
    tokenizer.texts_to_sequences(message_example),
    maxlen=max_len,
    padding=padding_type,
    truncating=trunc_type
)

pred = float(model.predict(message_example_tp))
if pred > threshold:
    print("This message is a real text")
else:
    print("This message is a spam message")





